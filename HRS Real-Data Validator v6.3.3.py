import subprocess
import sys

# --- è‡ªå‹•ç’°å¢ƒæª¢æŸ¥æ©Ÿåˆ¶ ---
def setup_environment():
    required = {"numpy", "pandas", "matplotlib", "scipy", "requests", "emcee", "corner"}
    try:
        import pkg_resources
        installed = {pkg.key for pkg in pkg_resources.working_set}
        missing = required - installed
        if missing:
            print(f"[*] åµæ¸¬åˆ°ç¼ºå¤±çµ„ä»¶: {missing}ï¼Œæ­£åœ¨è‡ªå‹•å®‰è£...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", *missing])
    except Exception:
        # é‡å° Colab ç’°å¢ƒçš„ç›¸å®¹è™•ç†
        pass

setup_environment()

# --- æ­£å¼å°å…¥ ---

import numpy as np
import pandas as pd
import emcee
import corner
import matplotlib.pyplot as plt
import os
import scipy.linalg as la

# ==========================================
# 1. ç©©å¥å‹æ•¸æ“šè¼‰å…¥ (å«æ­£è¦åŒ–è™•ç†)
# ==========================================
def load_data_calm():
    print("[*] v6.3.3 å†·éœç‰ˆå•Ÿå‹•ï¼šæ­£åœ¨è¼‰å…¥ Pantheon+ æ•¸æ“š...")
    
    dat_file = "Pantheon+SH0ES.dat"
    cov_file = "Pantheon+SH0ES_STAT+SYS.cov"
    
    if not (os.path.exists(dat_file) and os.path.exists(cov_file)):
        print("[âŒ] éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ•¸æ“šæª”æ¡ˆï¼Œè«‹ç¢ºèªæª”æ¡ˆå·²ä¸Šå‚³ã€‚")
        return None, None, None, None

    # è®€å–æ•¸æ“š
    df = pd.read_csv(dat_file, sep=r'\s+')
    
    # è®€å–å”æ–¹å·®çŸ©é™£ (è™•ç†æ¨™é ­)
    raw_data = np.fromfile(cov_file, sep=' ')
    n_header = int(raw_data[0])
    matrix_data = raw_data[1:]
    
    if len(matrix_data) != n_header**2:
        print("[!] æ¨™é ­æ ¼å¼ç•°å¸¸ï¼Œå˜—è©¦ç›´æ¥è®€å–...")
        matrix_data = raw_data # Fallback
        n_header = int(np.sqrt(len(raw_data)))

    cov_matrix = matrix_data.reshape((n_header, n_header))

    # æ•¸æ“šç¯©é¸ (z > 0.01 å®‡å®™å­¸æ¨™æº–)
    mask = (df['zHD'] > 0.01)
    df_clean = df[mask].reset_index(drop=True)
    z_obs = df_clean['zHD'].values
    mu_obs = df_clean['m_b_corr'].values
    
    # çŸ©é™£åˆ‡å‰²
    indices = df.index[mask].values
    cov_cut = cov_matrix[np.ix_(indices, indices)]
    
    # --- [é—œéµä¿®æ­£ 1] çŸ©é™£æ­£è¦åŒ– ---
    # å°è§’ç·šåŠ å…¥å¾®å°æ“¾å‹•ï¼Œé˜²æ­¢å¥‡ç•°å€¼å°è‡´ BIC çˆ†ç‚¸
    print("    -> æ­£åœ¨åŸ·è¡ŒçŸ©é™£æ­£è¦åŒ– (Regularization)...")
    cov_cut += np.eye(len(cov_cut)) * 1e-5
    
    # ä½¿ç”¨ Cholesky åˆ†è§£æ±‚åçŸ©é™£ (æ¯” np.linalg.inv æ›´ç©©å®š)
    try:
        # C = L * L.T => C^-1 = (L^-1).T * L^-1
        L = np.linalg.cholesky(cov_cut)
        inv_L = np.linalg.inv(L)
        inv_cov = np.dot(inv_L.T, inv_L)
    except np.linalg.LinAlgError:
        print("    [!] Cholesky åˆ†è§£å¤±æ•—ï¼Œé€€å›ä½¿ç”¨å½é€†çŸ©é™£ (Pinverse)...")
        inv_cov = np.linalg.pinv(cov_cut)
        
    print(f"[âœ…] æ•¸æ“šæº–å‚™å®Œæˆï¼š{len(z_obs)} é»ï¼ŒçŸ©é™£æ•¸å€¼å·²ç©©å®šåŒ–ã€‚")
    return z_obs, mu_obs, inv_cov, cov_cut

# ==========================================
# 2. ç‰©ç†æ¨¡å‹ (å¹¾ä½•å½¢ç‹€é æ¸¬)
# ==========================================
def theory_mu_shape(z, om, alpha, beta, model='lcdm'):
    # é€™è£¡æˆ‘å€‘ä¸å‚³å…¥ H0ï¼Œå› ç‚º H0 åªæ˜¯ä¸€å€‹å‚ç›´ä½ç§»
    # æˆ‘å€‘è¨ˆç®—çš„æ˜¯ "å½¢ç‹€" (Shape)ï¼Œä½ç§»ç”±è§£æè§£è™•ç†
    
    c = 299792.458
    # ç©åˆ†
    z_integ = np.linspace(0, np.max(z)*1.05, 1000)
    Ez = np.sqrt(om * (1 + z_integ)**3 + (1 - om))
    
    if model == 'hrs':
        # æŒ‡æ•¸è¡°æ¸›å…¨æ¯ä¿®æ­£
        correction = 1.0 + beta * np.exp(-z_integ / alpha)
        hz = Ez * correction # é€™è£¡æ²’æœ‰ H0ï¼Œå› ç‚ºå®ƒæ˜¯ç›¸å°è®ŠåŒ–
    else:
        hz = Ez
        
    inv_hz = 1.0 / hz
    dc_cum = np.cumsum(inv_hz) * (z_integ[1] - z_integ[0]) * c
    dc_interp = np.interp(z, z_integ, dc_cum)
    dl = (1 + z) * dc_interp
    
    # é€™è£¡çš„ mu = 5 log10(Dl) + 25
    # æˆ‘å€‘å…ˆç®—ä¸€å€‹ "åŸºç¤ mu" (å‡è¨­ H0=100)
    mu_base = 5.0 * np.log10(np.maximum(dl, 1e-10)) + 25.0
    return mu_base

# ==========================================
# 3. ä¼¼ç„¶å‡½æ•¸ (å«è§£æé‚Šéš›åŒ–ä¿®æ­£)
# ==========================================
def log_likelihood(theta, z, mu, inv_cov, model_type):
    # --- åƒæ•¸æ‹†è§£ ---
    if model_type == 'lcdm':
        om = theta[0]
        alpha = 1.0; beta = 0.0
    else:
        om, alpha, beta = theta
    
    # --- [é—œéµä¿®æ­£ 2] å¯¬å»£çš„åƒæ•¸å…ˆé©— (No Walls) ---
    if not (0.0 < om < 1.0): return -np.inf
    if model_type == 'hrs' and not (0.01 < alpha < 20.0 and -5.0 < beta < 5.0): return -np.inf

    # --- ç†è«–é æ¸¬ (Shape only) ---
    mu_model = theory_mu_shape(z, om, alpha, beta, model_type)
    
    # --- [é—œéµä¿®æ­£ 3] è§£æé‚Šéš›åŒ– (Analytical Marginalization) ---
    # æˆ‘å€‘ä¸æ“¬åˆ H0 èˆ‡ Mï¼Œè€Œæ˜¯ç”¨çŸ©é™£å…¬å¼è‡ªå‹•æ±‚å‡ºæœ€ä½³ä½ç§»
    # é€™æ˜¯æ¶ˆé™¤ "æ•¸å€¼åç§»ä½œå¼Š" çš„å”¯ä¸€æ–¹æ³•
    
    diff = mu - mu_model
    
    # å‘é‡è¨ˆç®—æ¬Šé‡
    W = np.sum(inv_cov)  # æ¬Šé‡ç¸½å’Œ
    if W == 0: return -np.inf
    
    weighted_diff = np.sum(np.dot(inv_cov, diff)) # åŠ æ¬Šåç§»
    delta = weighted_diff / W # é€™æ˜¯æœ€ä½³çš„ (H0 + M) ä½ç§»å€¼
    
    # ä¿®æ­£å¾Œçš„æ®˜å·®
    diff_corr = diff - delta
    
    # è¨ˆç®— Chi^2
    chisq = np.dot(diff_corr, np.dot(inv_cov, diff_corr))
    
    return -0.5 * chisq

# ==========================================
# 4. åŸ·è¡Œåˆ†æ
# ==========================================
if __name__ == "__main__":
    print("==========================================")
    print("   HRS v6.3.3 Calm Edition (No Artifacts)")
    print("==========================================")
    
    z, mu, inv_cov, cov = load_data_calm()
    
    if z is not None:
        nwalkers, steps = 32, 1200
        
        # --- åŸ·è¡Œ LCDM (1 åƒæ•¸: Omega_m) ---
        print("\n[*] åŸ·è¡Œ LCDM (åŸºæº–æ¨¡å‹)...")
        # åˆå§‹å€¼: Om=0.3
        sampler_l = emcee.EnsembleSampler(nwalkers, 1, log_likelihood, args=(z, mu, inv_cov, 'lcdm'))
        sampler_l.run_mcmc(0.3 + 1e-3*np.random.randn(nwalkers, 1), steps, progress=True)
        
        # --- åŸ·è¡Œ HRS (3 åƒæ•¸: Om, Alpha, Beta) ---
        print("\n[*] åŸ·è¡Œ HRS (å…¨æ¯ä¿®æ­£)...")
        # åˆå§‹å€¼: Om=0.3, Alpha=1.0, Beta=0.1
        pos_h = [0.3, 1.0, 0.1] + 1e-3*np.random.randn(nwalkers, 3)
        sampler_h = emcee.EnsembleSampler(nwalkers, 3, log_likelihood, args=(z, mu, inv_cov, 'hrs'))
        sampler_h.run_mcmc(pos_h, steps, progress=True)

        # --- çœŸå¯¦çµ±è¨ˆ ---
        def get_bic(sampler, k, n_data):
            lp = sampler.get_log_prob(discard=300, flat=True)
            max_log_like = np.max(lp)
            # BIC = k*ln(n) - 2*ln(L)
            return k * np.log(n_data) - 2*max_log_like, sampler.get_chain(discard=300, flat=True)[np.argmax(lp)]

        bic_l, theta_l = get_bic(sampler_l, 1, len(z)) # k=1 (Om only)
        bic_h, theta_h = get_bic(sampler_h, 3, len(z)) # k=3 (Om, Alpha, Beta)
        
        delta_bic = bic_l - bic_h # æ­£å€¼è¡¨ç¤ºæ”¯æŒ HRS

        print("\n" + "="*50)
        print("   HRS v6.3.3 çœŸå¯¦æ•¸æ“šåˆ†æå ±å‘Š")
        print("="*50)
        print(f" Delta BIC (çœŸå¯¦): {delta_bic:.4f}")
        print("-" * 50)
        print(f" HRS æœ€ä½³åƒæ•¸:")
        print(f" Omega_m : {theta_h[0]:.4f}")
        print(f" Alpha   : {theta_h[1]:.4f}")
        print(f" Beta    : {theta_h[2]:.4f}")
        print("-" * 50)
        
        if delta_bic > 6:
            print(" [çµè«–] å¼·çƒˆæ”¯æŒ (Strong Evidence, Delta BIC > 6)")
        elif delta_bic > 2:
            print(" [çµè«–] æ­£é¢æ”¯æŒ (Positive Evidence, Delta BIC > 2)")
        elif delta_bic > -2:
            print(" [çµè«–] å…©è€…ç„¡æ³•å€åˆ† (Inconclusive)")
        else:
            print(" [çµè«–] æ”¯æŒæ¨™æº–æ¨¡å‹ (Favor LCDM)")
            
        print("="*50)
        
        # ç¹ªåœ–
        labels = [r"$\Omega_m$", r"$\alpha$", r"$\beta$"]
        fig = corner.corner(sampler_h.get_chain(discard=300, flat=True), labels=labels, truths=theta_h, show_titles=True)
        plt.savefig("hrs_v6_3_3_calm.png")
        print("[ğŸ‰] åˆ†æå®Œæˆã€‚")

